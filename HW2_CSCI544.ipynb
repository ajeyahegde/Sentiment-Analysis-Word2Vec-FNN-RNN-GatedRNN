{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ad9b65",
   "metadata": {},
   "source": [
    "# CSCI544 - Homework Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8101c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ee902",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18fc7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data from file and extratcing review_body and star_ratings only\n",
    "df = pd.read_csv('data.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False, low_memory=False)\n",
    "data = df[['review_body', 'star_rating']]\n",
    "data = data[data['review_body'].str.len() > 0 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3742d",
   "metadata": {},
   "source": [
    "# Dataset Generation and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fb4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "\n",
    "data['star_rating'] = data['star_rating'].apply(int)\n",
    "\n",
    "#Separating the data based on star_rating\n",
    "\n",
    "rating_1 = data[data['star_rating']==1]\n",
    "rating_2 = data[data['star_rating']==2]\n",
    "rating_3 = data[data['star_rating']==3]\n",
    "rating_4 = data[data['star_rating']==4]\n",
    "rating_5 = data[data['star_rating']==5]\n",
    "\n",
    "\n",
    "\n",
    "#Taking a sample of 20000 from each rating value\n",
    "\n",
    "rating_1 = rating_1.sample(n=20000)\n",
    "rating_2 = rating_2.sample(n=20000)\n",
    "rating_3 = rating_3.sample(n=20000)\n",
    "rating_4 = rating_4.sample(n=20000)\n",
    "rating_5 = rating_5.sample(n=20000)\n",
    "\n",
    "#Forming a dataset of 100,000 rows of balanced ratings\n",
    "review_data = pd.concat([rating_1, rating_2,rating_3, rating_4,rating_5]) \n",
    "\n",
    "#Function for data cleaning\n",
    "def clean_review(text):\n",
    "    regex_alpha = '[^a-zA-Z]'\n",
    "    regex_html = '<.*?>'\n",
    "    regex_url = r'http[s]?://\\S+'\n",
    "    #Remove URL from reviews\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Remove HTML from reviews\n",
    "    text = re.sub(regex_html, ' ', text)\n",
    "    #Convert to lower letters\n",
    "    text = text.lower()\n",
    "    #Fix Contractions\n",
    "    text = contractions.fix(text)\n",
    "    #Remove non alphabetical characters\n",
    "    text = re.sub(regex_alpha, ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "    \n",
    "\n",
    "review_data[\"review_body\"] = review_data[\"review_body\"].apply(clean_review) \n",
    "review_data = review_data.sample(frac = 1)\n",
    "review_data[\"review_body\"] = review_data[\"review_body\"].apply(str)\n",
    "\n",
    "X = review_data.review_body\n",
    "Y = review_data.star_rating\n",
    "\n",
    "#Divide data into train and test set, train set has 80% of data and test set has 20% of data.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6a7b1",
   "metadata": {},
   "source": [
    "# Word Embedding - Google New Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b666c3",
   "metadata": {},
   "source": [
    "<p>Loaded Google Word2Vec model and performed semantic similarities for 3 examples as shown below</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cc4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading google news word2vec model\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4312c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7300517\n",
      "0.4135858\n",
      "0.08265579\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#Vector for king\n",
    "vec_king = wv['king']\n",
    "vec_queen = wv['queen']\n",
    "vec_man = wv['man']\n",
    "vec_woman = wv['woman']\n",
    "\n",
    "queen = vec_king-vec_man+vec_woman\n",
    "\n",
    "cos_sim1 = dot(vec_queen, queen)/(norm(vec_queen)*norm(queen))\n",
    "print(cos_sim1)\n",
    "\n",
    "vec_beautiful = wv['attractive']\n",
    "vec_attractive = wv['beautiful']\n",
    "\n",
    "cos_sim2 = dot(vec_beautiful, vec_attractive)/(norm(vec_beautiful)*norm(vec_attractive))\n",
    "print(cos_sim2)\n",
    "\n",
    "vec_good = wv['sweet']\n",
    "vec_bad = wv['player']\n",
    "\n",
    "cos_sim3 = dot(vec_good, vec_bad)/(norm(vec_good)*norm(vec_bad))\n",
    "print(cos_sim3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7a5c1",
   "metadata": {},
   "source": [
    "# Word Embedding - Word2Vec Model using own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10989829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "#Building own word2vec model with review data. Reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in X:\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=300, window=11,  min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e3bff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36439443\n",
      "0.5834363\n",
      "0.41000557\n"
     ]
    }
   ],
   "source": [
    "vec_king1 = model.wv['king']\n",
    "vec_queen1 = model.wv['queen']\n",
    "vec_man1 = model.wv['man']\n",
    "vec_woman1 = model.wv['woman']\n",
    "\n",
    "queen1 = vec_king1-vec_man1+vec_woman1\n",
    "\n",
    "cos_sim1 = dot(vec_queen1, queen1)/(norm(vec_queen1)*norm(queen1))\n",
    "print(cos_sim1)\n",
    "\n",
    "vec_beautiful1 = model.wv['beautiful']\n",
    "vec_attractive1 = model.wv['attractive']\n",
    "\n",
    "cos_sim2 = dot(vec_beautiful1, vec_attractive1)/(norm(vec_beautiful1)*norm(vec_attractive1))\n",
    "print(cos_sim2)\n",
    "\n",
    "vec_good1 = model.wv['sweet']\n",
    "vec_bad1 = model.wv['player']\n",
    "\n",
    "cos_sim3 = dot(vec_good1, vec_bad1)/(norm(vec_good1)*norm(vec_bad1))\n",
    "print(cos_sim3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9de0d",
   "metadata": {},
   "source": [
    "<h2> Conclusion/Answers to Q2</h2>\n",
    "\n",
    "<h3>I see vectors generated by the pre-trained model maintained the semantic similarities better becuase our word2vec model was trained using less data. More the data, better is the model. From the cosine similarites for operations King-Man+Woman and Queen, the similarity is more with Google Word2Vec model. Both models maintained similar semantic similarity for words beautiful and attractive. But for words sweet and player which is not related and hardly seen together in context, pretrained model perfomed better to give the low value since they are not related but my model gave a higher similarity value for that.\n",
    "So Google Word2Vec model performed better for my example and that's because it is trained on more data.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d60609",
   "metadata": {},
   "source": [
    "# Q3 - Feature Extraction using Word2Vec and Simple Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f2482",
   "metadata": {},
   "source": [
    "<p>For this question, I extracted features for reviews using average Word2Vec vectors for each review as the input. The feature set generated using average Word2Vec is used for training the Perceptron and Support Vector machine model and predictions are made. Below I'm printing the accuracies and comparing the accuracies with same model with features from TF-IDF from previous assignment and Word2Vec features we extracted here</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fadf06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Form feature vectors for training SVM and Perceptron\n",
    "\n",
    "def extractFeature(review):\n",
    "    words = review.split(\" \")\n",
    "    sentence_vector = np.zeros(300)\n",
    "    for word in words:\n",
    "        if word in  wv.key_to_index:\n",
    "            word_vector = wv[word]\n",
    "            sentence_vector = sentence_vector + word_vector\n",
    "    sentence_vector = sentence_vector / len(words) \n",
    "    return sentence_vector\n",
    "\n",
    "X_train_feature = list(X_train.apply(extractFeature))\n",
    "#print(len(X_train_feature))\n",
    "X_test_feature = list(X_test.apply(extractFeature))\n",
    "#print(len(X_test_feature))\n",
    "\n",
    "X_train_feature1 = X_train.apply(extractFeature)\n",
    "X_test_feature1 = X_test.apply(extractFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64b912",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8623669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron Model using TF-IDF feature: 0.39 \n",
      "Accuracy of Perceptron Model using Word2Vec feature: 0.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics\n",
    "\n",
    "perceptron = Perceptron(\n",
    "               max_iter=250,\n",
    "               tol=0.001)\n",
    "perceptron.fit(list(X_train_feature), Y_train)\n",
    "prediction_perceptron = perceptron.predict(list(X_test_feature))\n",
    "perceptron_results = metrics.classification_report(Y_test, prediction_perceptron, output_dict=True)\n",
    "accuracy = str(round(perceptron_results['accuracy'],2))\n",
    "\n",
    "print(\"Accuracy of Perceptron Model using TF-IDF feature: 0.39 \")\n",
    "print(\"Accuracy of Perceptron Model using Word2Vec feature:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d504f",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8885773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Model using TF-IDF feature: 0.47 \n",
      "Accuracy of SVM Model using Word2Vec feature: 0.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Fitting SVM model and predicting the ratings\n",
    "SVCmodel = LinearSVC(C = 0.5)\n",
    "SVCmodel.fit(X_train_feature, Y_train)\n",
    "prediction_svm = SVCmodel.predict(X_test_feature)\n",
    "\n",
    "print(\"Accuracy of SVM Model using TF-IDF feature: 0.47 \")\n",
    "svm_results = metrics.classification_report(Y_test, prediction_svm, output_dict=True)\n",
    "accuracy = str(round(svm_results['accuracy'],2))\n",
    "\n",
    "print(\"Accuracy of SVM Model using Word2Vec feature:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c078771",
   "metadata": {},
   "source": [
    "<h2>Conclusion and Answer to Q3</h2>\n",
    "<h3>For simple models, I'm comparing the accuracy for Perceptron and Support Vector Machine trained on Tf-Idf model from homework 1 and same models trained with word2vec features from this homework. We can see the for perceptron, the accuracy was better for model trained with TF-IDF features but for Support Vector Machines, the accuracy was better with Word2Vec features than the Tf-Idf features. Since perceptron is a simple single layer neural network, the model needs more data to learn.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4763b",
   "metadata": {},
   "source": [
    "# DataSet preparation and Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e412db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming labels into one-hot vector form for training the neural network models\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train)\n",
    "Y_test = pd.get_dummies(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc48d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajeya\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Transforming features and labels to tensors, forming a dataset using tensors and loading \n",
    "# the dataset to DataLoader for training the Neural Network\n",
    "\n",
    "train_feature_tensor = torch.tensor(X_train_feature, dtype=torch.float32)\n",
    "test_feature_tensor = torch.tensor(X_test_feature, dtype=torch.float32)\n",
    "train_label = torch.tensor(Y_train.values, dtype=torch.float32)\n",
    "test_label = torch.tensor(Y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_data = TensorDataset(train_feature_tensor, train_label)\n",
    "test_data = TensorDataset(test_feature_tensor, test_label)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75966a58",
   "metadata": {},
   "source": [
    "# Q4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe01882a",
   "metadata": {},
   "source": [
    "<p>Below class is for Feedforward neural network. It has two hidden layers of 50 and 10 nodes respectively. Input layer has 300 nodes to take 300-dim vector from word2vec feature as input and output layer has 5 nodes for 5 different classes for labels. I'm using cross entropy loss and SGD optimizer for training the neural network. \n",
    "    \n",
    "Reference: <a>https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook</a> </p>\n",
    "\n",
    "<h3>Q4-(a)</h3>\n",
    "<p> For this question, the feature vector is the weighted average of all the word2vec vector for each word in the review.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d78054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#This is the class for Feedforward neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 300)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=0)\n",
    "\n",
    "model = Net()\n",
    "#print(model)    \n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train() \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    #print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e0ae4",
   "metadata": {},
   "source": [
    "<p> In below tab, I'm predicting the labels for reviews in testing set and calculating the accuracy of the neural network.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60bd936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4-(a)Accuracy of Neural Network: 0.4786\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        #print(output)\n",
    "        for index, i in enumerate(output):\n",
    "            if torch.argmax(i) == np.argmax(target[index]):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "#print(total)\n",
    "#print(correct)\n",
    "print(\"Q4-(a)Accuracy of Neural Network:\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d621646",
   "metadata": {},
   "source": [
    "<h3>Q4-(b)</h3>\n",
    "\n",
    "<p> In the below model,I'm changing the way feature is being generated. In previous question feature was generated using weighted average of all the word2vec vectors of the words in review. In this question, I'm concatenating the word2vec vector for first 10 words of the reviews (If review has less words, I'm concatenating zero vector for the feature. Due to this the input layer of the neural network should have 3000 nodes as our input feature is 3000-dim vector.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a437a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetConcat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetConcat, self).__init__()\n",
    "        self.fc1 = nn.Linear(3000, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3000)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def extractFeatureConcat(review):\n",
    "    words = review.split(\" \")\n",
    "    words = words[0:10]\n",
    "    sentence_vector = np.array([])\n",
    "    for i in range(10):\n",
    "        if(i < len(words) and words[i] in wv.key_to_index):\n",
    "            word_vector = wv[words[i]]\n",
    "            sentence_vector = np.concatenate([sentence_vector, word_vector])\n",
    "        else:\n",
    "            sentence_vector = np.concatenate([sentence_vector, np.zeros(300)])\n",
    "    return sentence_vector\n",
    "\n",
    "X_train_feature_concat = list(X_train.apply(extractFeatureConcat))\n",
    "#print(len(X_train_feature_concat))\n",
    "X_test_feature_concat = list(X_test.apply(extractFeatureConcat))\n",
    "#print(len(X_test_feature_concat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2adda8",
   "metadata": {},
   "source": [
    "<p>Below is the code for generating feature vector by concatenating word2vec vectors of first 10 words in the review.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2bf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_concat_tensor = torch.tensor(X_train_feature_concat, dtype=torch.float32)\n",
    "test_feature_concat_tensor = torch.tensor(X_test_feature_concat, dtype=torch.float32)\n",
    "train_label = torch.tensor(Y_train.values, dtype=torch.float32)\n",
    "test_label = torch.tensor(Y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_data_concat = TensorDataset(train_feature_concat_tensor, train_label)\n",
    "test_data_concat = TensorDataset(test_feature_concat_tensor, test_label)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader_concat = DataLoader(train_data_concat, batch_size = batch_size, shuffle = True)\n",
    "test_loader_concat = DataLoader(test_data_concat, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc3e9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_concat = NetConcat()\n",
    "#print(model_concat)    \n",
    "\n",
    "#Using CrossEntropyLoss for training and SGD for optimization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model_concat.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "#Training the model using the features generated.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model_concat.train() \n",
    "    for data, target in train_loader_concat:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_concat(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader_concat.dataset)\n",
    "    #print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3f7e3",
   "metadata": {},
   "source": [
    "<p> Below is the code for predicting labels for reviews in testset using second neural network trained on feature vectors formed by concatenating the vectors of first 10 words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6850786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4-(b)Accuracy of Neural Network: : 0.39815\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader_concat:\n",
    "        output = model_concat(data)\n",
    "        for index, i in enumerate(output):\n",
    "            if torch.argmax(i) == np.argmax(target[index]):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "#print(total)\n",
    "#print(correct)\n",
    "print(\"Q4-(b)Accuracy of Neural Network: :\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02081092",
   "metadata": {},
   "source": [
    "<h2>Conclusion and answer to Q4</h2>\n",
    "<h3>\n",
    "We see that the accuracy for feedforward neural networks is slightly lower than accuracy of SVM because nerual networks learns and performs well with more data. Since we had only 100,000 records, we could see a decent performance but we need more data to adjust right weights. We also see that neural networks performed better than the perceptron since neural networks have more layers and nodes than the perceptron which helps in learning better. Overall, more data would be useful for neural networks to learn.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fbbaa",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60276a",
   "metadata": {},
   "source": [
    "<p>Below is the class for recurrent neural network. For my model, as mentioned in Assignment description, I'm using hidden state size as 20, input layer size is 300 for 300-dim feature vector extracted from word2vec model. For each review, I use first feature vector of first 20 words to train the RNN model.</p>\n",
    "<br>\n",
    "References:<br>\n",
    "<a>https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html</a><br>\n",
    "<a>https://pytorch.org/docs/stable/generated/torch.nn.GRU.html</a>\n",
    "<br>\n",
    "<a>https://pytorch.org/docs/stable/generated/torch.nn.RNN.html</a>\n",
    "<br>\n",
    "<h3>Q5-(a)</h3>\n",
    "For this question, class RNN is the recurrent neural network model I train and use for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7866a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing dataset for training RNN\n",
    "X_train_rnn = X_train.values\n",
    "Y_train_rnn = Y_train.values\n",
    "\n",
    "X_test_rnn = X_test.values\n",
    "Y_test_rnn = Y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb57e69",
   "metadata": {},
   "source": [
    "<p> Below is the code to form the dataset and dataloader for traing the RNN and gated RNN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59cd41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below function takes review as input and returns numpy array of feature vector as output.\n",
    "def review_to_vector(review):\n",
    "    words = review.split(\" \")\n",
    "    review_size = len(words)\n",
    "    word_vector = np.zeros(shape=(20, 300))\n",
    "    for i in range(0,20):\n",
    "        if(review_size > i and words[i] in  wv.key_to_index):\n",
    "            word_vector[i] = wv[words[i]]\n",
    "    return word_vector\n",
    "\n",
    "#Below is the code for forming feature dataset tensor for training the Gated RNN\n",
    "Y_train_rnn = torch.tensor(Y_train_rnn, dtype=torch.float32)\n",
    "Y_test_rnn = torch.tensor(Y_test_rnn, dtype=torch.float32)\n",
    "X_train_rnn = np.array(list(map(review_to_vector, X_train_rnn)))\n",
    "X_test_rnn = np.array(list(map(review_to_vector, X_test_rnn)))\n",
    "\n",
    "X_train_rnn_tensor = torch.tensor(X_train_rnn, dtype=torch.float32)\n",
    "X_test_rnn_tensor = torch.tensor(X_test_rnn, dtype=torch.float32)\n",
    "\n",
    "#Form Tensor Dataset\n",
    "train_data_rnn = TensorDataset(X_train_rnn_tensor, Y_train_rnn)\n",
    "test_data_rnn = TensorDataset(X_test_rnn_tensor, Y_test_rnn)\n",
    "\n",
    "#Load the dataset to DataLoader\n",
    "batch_size = 50\n",
    "train_loader_rnn = DataLoader(train_data_rnn, batch_size = batch_size, shuffle = True)\n",
    "test_loader_rnn = DataLoader(test_data_rnn, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04d5c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for RNN model \n",
    "#Reference: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, layers, input_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        #RNN has 300 as input size to take 300-dim feature vector and 5 outputsize for 5 different classes\n",
    "        self.rnn = nn.RNN(input_size, 300, layers,batch_first=True)\n",
    "        self.h0 = torch.randn(layers, 100, 300)\n",
    "        self.linear = nn. Linear(300, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.rnn(input_tensor)[0][:,-1]\n",
    "        #Since there are 5 classes, a linear layer is used to convert from 300 size to size 5 for output\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "rnn_model = RNN(2, 300, 5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    for data, target in train_loader_rnn:\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader_rnn.dataset)\n",
    "    #print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8060f145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5-(a) Accuracy of RNN model: 0.4351\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader_rnn:\n",
    "        output = rnn_model(data)\n",
    "        for index, i in enumerate(output):\n",
    "            if torch.argmax(i) == np.argmax(target[index]):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "#print(correct)\n",
    "#print(total)\n",
    "print(\"Q5-(a) Accuracy of RNN model:\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40dcf00",
   "metadata": {},
   "source": [
    "<h2>Conclusion and answer to Q5-a</h2>\n",
    "\n",
    "<h3>\n",
    "    We see that the accuracy of RNN has decreased when compared to feedforward neural network. The reason can be found by understanding data better. RNN learns from the sequence of data, which is sequence of words here. Since there was less data, Feedforward neural network worked better than RNN, but with more data it might be possible that RNN learns better than the feedforward.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d49a8f",
   "metadata": {},
   "source": [
    "<h3>Q5-(b)</h3>\n",
    "\n",
    "<p>GRNN class is a class built for gated RNN. It has 300 nodes as input, 300 layers for hidden state and output size is 5 since we have 5 classes to predict.<br>\n",
    "Train Label Shape: [80000, 5] <br>\n",
    "Test Label Shape: [20000, 5]<br>\n",
    "Train Feature Shape: [80000, 20, 300]<br>\n",
    "Test Feature Shape: [20000, 20, 300]<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ad2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is the class for Gated RNN \n",
    "#Reference: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    def __init__(self, layers, input_size, output_size):\n",
    "        super(GRNN, self).__init__()\n",
    "        #Input size is 300 to feed feature vectors, network has 2 layers and output size is 5 since there are 5 classes.\n",
    "        self.gru = nn.GRU(input_size, 300, layers,batch_first=True)\n",
    "        self.h0 = torch.randn(layers, 100, 300)\n",
    "        self.linear = nn. Linear(300, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.gru(input_tensor)[0][:,-1]\n",
    "        #Output from the GRU is passed to next layer further get 5 output values\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "gated_rnn = GRNN(2, 300, 5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(gated_rnn.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40559a4f",
   "metadata": {},
   "source": [
    "<p>Below code form featureset used for training the Gated RNN model. For each review, a feature is formed by using first 20 words and converting it to word2vec feature vector</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5df6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader_gru = DataLoader(train_data_rnn, batch_size = batch_size, shuffle = True)\n",
    "test_loader_gru = DataLoader(test_data_rnn, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(gated_rnn.parameters(), lr=0.01)\n",
    "\n",
    "train_loss = 0\n",
    "epochs = 5\n",
    "\n",
    "#For loop for training the Gated RNN (GRNN) model \n",
    "for i in range(epochs):\n",
    "\n",
    "    for data, target in train_loader_gru:\n",
    "        optimizer.zero_grad()\n",
    "        output = gated_rnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader_gru.dataset)\n",
    "    #print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da1160",
   "metadata": {},
   "source": [
    "<p>Below code is for predicting the labels for test dataset using the Gated RNN model. Accuracy is as shown below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b1fc26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5-(b) Accuracy of gated RNN model: 0.3791\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#Below loop is used to calculate correct and total labels for gated RNN model to calculate accuracy of the model\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader_gru:\n",
    "        output = gated_rnn(data)\n",
    "        for index, i in enumerate(output):\n",
    "            if torch.argmax(i) == np.argmax(target[index]):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Q5-(b) Accuracy of gated RNN model:\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f16f98",
   "metadata": {},
   "source": [
    "<h2>Conclusion and answer to Q5-b </h2>\n",
    "\n",
    "<h3> There is not huge difference between accuracies of RNN and gated RNN. Gated RNN can sometimes remember better than the RNN for sequence data. But it is hard to analyze if gated RNN or RNN is better for this problem since review length varies with rows. As seen previously, the gated RNN model did not over perform feedforward NN for this problem with the data we have. For this problem RNN performed better than gated RNN. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d113b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "print(np.version.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
